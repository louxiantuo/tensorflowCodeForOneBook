{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tenor](pictures/tenor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Image Evaluation in Medical Clinic Diagnosis\n",
    "\n",
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">\n",
    "The use of AI in medical treatment is becoming a powerful weapon for epidemic prevention and control. In the past, doctors who need long-term training can do professional diagnosis. Now, through the latest artificial intelligence identification system, we can diagnose the CT image of patients, and analyze the human cells infected by pathogens, so that in hospitals where medical experts and equipment are insufficient, we can also diagnose the epidemic situation in an emergency. First of all, AI is used to do basic screening and judgment, and then doctors can do control and correction, to guarantee life safety for the patients and improve the diagnosis efficiency of doctors.\n",
    "  \n",
    "</code>\n",
    "</p>\n",
    "\n",
    "## Images discrimination and generation by GAN\n",
    "\n",
    "\n",
    "* Generative Adversarial Network (GAN) is one of the most popular Machine Learning methods developed in recent years. At a high level, GAN is composed with two neural networks: the generator and the discriminator. A Generator (“the artist”) learns to create images that look real, while a Discriminator (“the art critic”) learns to tell real images apart from fakes.\n",
    "* During the training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.\n",
    "\n",
    "\n",
    "\n",
    "![Gan1](pictures/Gan1.png)\n",
    "\n",
    "\n",
    "## Double structures of GAN model\n",
    "\n",
    "* The structure of GAN mainly includes a Generator G and a Discriminator D. The purpose of the generator G during the training process is to learn the real data distribution to make the generated pictures more reliable, and thus \"deceived\" the discriminator. The goal of discriminator D is to separate the pictures generated by G from the real pictures as much as possible. In this way, G and D constitute a dynamic \"game process\". The whole process is that the pictures generated by the generator are more and more real, and the discriminator's performance in discriminating real pictures is more accurate. As time goes by, the two models reach an ideal balance, and G can generate \"fake and untrue\" pictures. For D, it is difficult to determine whether the picture generated by G is a real picture.\n",
    "* Take the GAN network to generate handwritten numbers as an example, initially, the generator accepts a random noise Z, through this noise to generate a digital image, and then the discriminator is to determine whether the generated image or the real image, in the early training, the generator generated samples that are easily discarded by the discriminator, as the number of iterations increases, the generator will generate more and more similar to human handwritten numbers, and the discriminator discriminates more accurately.\n",
    "* Accordingly, let’s say we want to generate the CT image that would pass the discriminator as authentic. First, we have to ask the generator to produce fake images of the MRI image. Then we have to pass both the fake CT image and the real CT image into the discriminator. The discriminator then goes to work on sorting the real from the fake. The following is the example of source code in TensorFlow. These two functions are defined as the generator and discriminator. It is important to note that both functions have two hidden layers and are connected. The first code is illustrated as the generator,\n",
    "\n",
    "\n",
    "![Gan1](pictures/gan2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Import the needed library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Initinilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAN(): \n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "## The Generator part in GAN\n",
    "<p style=\"background:DodgerBlue\">\n",
    "<code style=\"background:DodgerBlue;color:white\">\n",
    "model.add(Dense(256, input_dim=self.latent_dim)), it means 256 neurons with input dimension in the FIRST hidden layer of generator part.\n",
    "    \n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN): \n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## The Discriminator part in GAN\n",
    "<p style=\"background:DodgerBlue\">\n",
    "<code style=\"background:DodgerBlue;color:white\">\n",
    "model.add(Dense(512)), it means 512 neurons with input dimension in the FIRST hidden layer in discriminator part.\n",
    "    \n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN): \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## The Training part in GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN): \n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN): \n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## The execution time of GAN\n",
    "<p style=\"background:DodgerBlue\">\n",
    "<code style=\"background:DodgerBlue;color:white\">\n",
    "Don't modify the epochs number, the number is concerning to the system learning/execution time.\n",
    "    \n",
    "</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.640862, acc.: 45.31%] [G loss: 0.446234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.373048, acc.: 71.88%] [G loss: 0.458716]\n",
      "2 [D loss: 0.383365, acc.: 67.19%] [G loss: 0.576796]\n",
      "3 [D loss: 0.352552, acc.: 76.56%] [G loss: 0.647508]\n",
      "4 [D loss: 0.359930, acc.: 81.25%] [G loss: 0.757542]\n",
      "5 [D loss: 0.322538, acc.: 84.38%] [G loss: 0.872035]\n",
      "6 [D loss: 0.254908, acc.: 96.88%] [G loss: 1.074164]\n",
      "7 [D loss: 0.219347, acc.: 100.00%] [G loss: 1.252729]\n",
      "8 [D loss: 0.181047, acc.: 100.00%] [G loss: 1.384701]\n",
      "9 [D loss: 0.150271, acc.: 100.00%] [G loss: 1.630984]\n",
      "10 [D loss: 0.127544, acc.: 100.00%] [G loss: 1.699853]\n",
      "11 [D loss: 0.111583, acc.: 100.00%] [G loss: 1.900062]\n",
      "12 [D loss: 0.109445, acc.: 100.00%] [G loss: 2.010497]\n",
      "13 [D loss: 0.087252, acc.: 100.00%] [G loss: 2.216022]\n",
      "14 [D loss: 0.067167, acc.: 100.00%] [G loss: 2.336895]\n",
      "15 [D loss: 0.058170, acc.: 100.00%] [G loss: 2.489378]\n",
      "16 [D loss: 0.062947, acc.: 100.00%] [G loss: 2.445242]\n",
      "17 [D loss: 0.050396, acc.: 100.00%] [G loss: 2.604207]\n",
      "18 [D loss: 0.046344, acc.: 100.00%] [G loss: 2.637858]\n",
      "19 [D loss: 0.045713, acc.: 100.00%] [G loss: 2.717116]\n",
      "20 [D loss: 0.050781, acc.: 100.00%] [G loss: 2.811136]\n",
      "21 [D loss: 0.053851, acc.: 100.00%] [G loss: 3.065933]\n",
      "22 [D loss: 0.039821, acc.: 100.00%] [G loss: 2.987118]\n",
      "23 [D loss: 0.033197, acc.: 100.00%] [G loss: 3.071832]\n",
      "24 [D loss: 0.031181, acc.: 100.00%] [G loss: 3.160242]\n",
      "25 [D loss: 0.038090, acc.: 100.00%] [G loss: 3.188837]\n",
      "26 [D loss: 0.028633, acc.: 100.00%] [G loss: 3.445496]\n",
      "27 [D loss: 0.026477, acc.: 100.00%] [G loss: 3.509114]\n",
      "28 [D loss: 0.022086, acc.: 100.00%] [G loss: 3.395462]\n",
      "29 [D loss: 0.025567, acc.: 100.00%] [G loss: 3.478112]\n",
      "30 [D loss: 0.025485, acc.: 100.00%] [G loss: 3.421057]\n",
      "31 [D loss: 0.021582, acc.: 100.00%] [G loss: 3.696766]\n",
      "32 [D loss: 0.021066, acc.: 100.00%] [G loss: 3.525129]\n",
      "33 [D loss: 0.022560, acc.: 100.00%] [G loss: 3.427152]\n",
      "34 [D loss: 0.017063, acc.: 100.00%] [G loss: 3.755821]\n",
      "35 [D loss: 0.016970, acc.: 100.00%] [G loss: 3.619246]\n",
      "36 [D loss: 0.020129, acc.: 100.00%] [G loss: 3.610006]\n",
      "37 [D loss: 0.021843, acc.: 100.00%] [G loss: 3.655129]\n",
      "38 [D loss: 0.014814, acc.: 100.00%] [G loss: 3.640574]\n",
      "39 [D loss: 0.017790, acc.: 100.00%] [G loss: 3.642295]\n",
      "40 [D loss: 0.016743, acc.: 100.00%] [G loss: 3.588794]\n",
      "41 [D loss: 0.025356, acc.: 100.00%] [G loss: 3.847286]\n",
      "42 [D loss: 0.016307, acc.: 100.00%] [G loss: 3.915443]\n",
      "43 [D loss: 0.016000, acc.: 100.00%] [G loss: 3.821903]\n",
      "44 [D loss: 0.017617, acc.: 100.00%] [G loss: 3.767815]\n",
      "45 [D loss: 0.018557, acc.: 100.00%] [G loss: 3.927174]\n",
      "46 [D loss: 0.018103, acc.: 100.00%] [G loss: 4.017395]\n",
      "47 [D loss: 0.015959, acc.: 100.00%] [G loss: 4.011043]\n",
      "48 [D loss: 0.018692, acc.: 100.00%] [G loss: 4.054574]\n",
      "49 [D loss: 0.008699, acc.: 100.00%] [G loss: 4.098817]\n",
      "50 [D loss: 0.014171, acc.: 100.00%] [G loss: 4.052305]\n",
      "51 [D loss: 0.013480, acc.: 100.00%] [G loss: 4.131095]\n",
      "52 [D loss: 0.018005, acc.: 100.00%] [G loss: 4.176307]\n",
      "53 [D loss: 0.012602, acc.: 100.00%] [G loss: 4.282475]\n",
      "54 [D loss: 0.013730, acc.: 100.00%] [G loss: 4.196475]\n",
      "55 [D loss: 0.014222, acc.: 100.00%] [G loss: 4.140176]\n",
      "56 [D loss: 0.014537, acc.: 100.00%] [G loss: 4.125980]\n",
      "57 [D loss: 0.015112, acc.: 100.00%] [G loss: 4.288997]\n",
      "58 [D loss: 0.012082, acc.: 100.00%] [G loss: 4.360566]\n",
      "59 [D loss: 0.012788, acc.: 100.00%] [G loss: 4.301033]\n",
      "60 [D loss: 0.014296, acc.: 100.00%] [G loss: 4.271109]\n",
      "61 [D loss: 0.013971, acc.: 100.00%] [G loss: 4.334299]\n",
      "62 [D loss: 0.009214, acc.: 100.00%] [G loss: 4.449117]\n",
      "63 [D loss: 0.012252, acc.: 100.00%] [G loss: 4.526646]\n",
      "64 [D loss: 0.010218, acc.: 100.00%] [G loss: 4.336090]\n",
      "65 [D loss: 0.008931, acc.: 100.00%] [G loss: 4.507638]\n",
      "66 [D loss: 0.010266, acc.: 100.00%] [G loss: 4.353288]\n",
      "67 [D loss: 0.011137, acc.: 100.00%] [G loss: 4.309275]\n",
      "68 [D loss: 0.013317, acc.: 100.00%] [G loss: 4.429138]\n",
      "69 [D loss: 0.015628, acc.: 100.00%] [G loss: 4.535960]\n",
      "70 [D loss: 0.014946, acc.: 100.00%] [G loss: 4.708652]\n",
      "71 [D loss: 0.011488, acc.: 100.00%] [G loss: 4.634550]\n",
      "72 [D loss: 0.010453, acc.: 100.00%] [G loss: 4.707643]\n",
      "73 [D loss: 0.013220, acc.: 100.00%] [G loss: 4.720224]\n",
      "74 [D loss: 0.012970, acc.: 100.00%] [G loss: 4.643093]\n",
      "75 [D loss: 0.009316, acc.: 100.00%] [G loss: 4.650720]\n",
      "76 [D loss: 0.009428, acc.: 100.00%] [G loss: 4.581261]\n",
      "77 [D loss: 0.009544, acc.: 100.00%] [G loss: 4.605408]\n",
      "78 [D loss: 0.009335, acc.: 100.00%] [G loss: 4.633538]\n",
      "79 [D loss: 0.013858, acc.: 100.00%] [G loss: 4.666051]\n",
      "80 [D loss: 0.012982, acc.: 100.00%] [G loss: 4.755712]\n",
      "81 [D loss: 0.008974, acc.: 100.00%] [G loss: 4.777029]\n",
      "82 [D loss: 0.011257, acc.: 100.00%] [G loss: 4.886459]\n",
      "83 [D loss: 0.012554, acc.: 100.00%] [G loss: 4.893514]\n",
      "84 [D loss: 0.008224, acc.: 100.00%] [G loss: 4.819999]\n",
      "85 [D loss: 0.014390, acc.: 100.00%] [G loss: 4.814788]\n",
      "86 [D loss: 0.008983, acc.: 100.00%] [G loss: 4.927896]\n",
      "87 [D loss: 0.008884, acc.: 100.00%] [G loss: 4.815958]\n",
      "88 [D loss: 0.011104, acc.: 100.00%] [G loss: 4.993695]\n",
      "89 [D loss: 0.010618, acc.: 100.00%] [G loss: 4.924274]\n",
      "90 [D loss: 0.006704, acc.: 100.00%] [G loss: 4.986044]\n",
      "91 [D loss: 0.009492, acc.: 100.00%] [G loss: 4.843385]\n",
      "92 [D loss: 0.007256, acc.: 100.00%] [G loss: 4.878176]\n",
      "93 [D loss: 0.015780, acc.: 100.00%] [G loss: 5.045812]\n",
      "94 [D loss: 0.009487, acc.: 100.00%] [G loss: 5.068332]\n",
      "95 [D loss: 0.015116, acc.: 100.00%] [G loss: 4.904746]\n",
      "96 [D loss: 0.007182, acc.: 100.00%] [G loss: 4.814145]\n",
      "97 [D loss: 0.012475, acc.: 100.00%] [G loss: 4.905596]\n",
      "98 [D loss: 0.014808, acc.: 100.00%] [G loss: 5.078746]\n",
      "99 [D loss: 0.013325, acc.: 100.00%] [G loss: 5.021783]\n",
      "100 [D loss: 0.010333, acc.: 100.00%] [G loss: 5.317459]\n",
      "101 [D loss: 0.011796, acc.: 100.00%] [G loss: 5.354591]\n",
      "102 [D loss: 0.007516, acc.: 100.00%] [G loss: 5.094251]\n",
      "103 [D loss: 0.013968, acc.: 100.00%] [G loss: 5.249127]\n",
      "104 [D loss: 0.006957, acc.: 100.00%] [G loss: 5.269107]\n",
      "105 [D loss: 0.012102, acc.: 100.00%] [G loss: 5.214272]\n",
      "106 [D loss: 0.010560, acc.: 100.00%] [G loss: 5.303895]\n",
      "107 [D loss: 0.008349, acc.: 100.00%] [G loss: 5.175424]\n",
      "108 [D loss: 0.010545, acc.: 100.00%] [G loss: 5.281807]\n",
      "109 [D loss: 0.011603, acc.: 100.00%] [G loss: 5.325333]\n",
      "110 [D loss: 0.019980, acc.: 100.00%] [G loss: 5.250080]\n",
      "111 [D loss: 0.007582, acc.: 100.00%] [G loss: 5.169808]\n",
      "112 [D loss: 0.012666, acc.: 100.00%] [G loss: 5.128922]\n",
      "113 [D loss: 0.054396, acc.: 96.88%] [G loss: 5.989474]\n",
      "114 [D loss: 1.111596, acc.: 56.25%] [G loss: 2.715983]\n",
      "115 [D loss: 1.098247, acc.: 71.88%] [G loss: 2.782899]\n",
      "116 [D loss: 0.438735, acc.: 84.38%] [G loss: 2.810740]\n",
      "117 [D loss: 0.313693, acc.: 84.38%] [G loss: 3.010270]\n",
      "118 [D loss: 0.120061, acc.: 93.75%] [G loss: 3.478767]\n",
      "119 [D loss: 0.046543, acc.: 100.00%] [G loss: 3.920683]\n",
      "120 [D loss: 0.098366, acc.: 95.31%] [G loss: 4.007157]\n",
      "121 [D loss: 0.054007, acc.: 98.44%] [G loss: 3.917602]\n",
      "122 [D loss: 0.049746, acc.: 98.44%] [G loss: 3.793352]\n",
      "123 [D loss: 0.056627, acc.: 98.44%] [G loss: 4.260714]\n",
      "124 [D loss: 0.057523, acc.: 98.44%] [G loss: 4.244161]\n",
      "125 [D loss: 0.086133, acc.: 96.88%] [G loss: 4.574357]\n",
      "126 [D loss: 0.031626, acc.: 100.00%] [G loss: 4.634995]\n",
      "127 [D loss: 0.022216, acc.: 100.00%] [G loss: 4.624700]\n",
      "128 [D loss: 0.149381, acc.: 96.88%] [G loss: 4.222639]\n",
      "129 [D loss: 0.028193, acc.: 100.00%] [G loss: 4.220719]\n",
      "130 [D loss: 0.055907, acc.: 98.44%] [G loss: 4.341706]\n",
      "131 [D loss: 0.070487, acc.: 100.00%] [G loss: 3.596910]\n",
      "132 [D loss: 0.065313, acc.: 98.44%] [G loss: 3.688029]\n",
      "133 [D loss: 0.041742, acc.: 100.00%] [G loss: 3.925649]\n",
      "134 [D loss: 0.091022, acc.: 98.44%] [G loss: 3.949350]\n",
      "135 [D loss: 0.089536, acc.: 98.44%] [G loss: 3.736423]\n",
      "136 [D loss: 0.106896, acc.: 96.88%] [G loss: 4.130718]\n",
      "137 [D loss: 0.049324, acc.: 100.00%] [G loss: 3.740477]\n",
      "138 [D loss: 0.090780, acc.: 96.88%] [G loss: 3.880489]\n",
      "139 [D loss: 0.106493, acc.: 96.88%] [G loss: 3.850544]\n",
      "140 [D loss: 0.126353, acc.: 95.31%] [G loss: 3.817719]\n",
      "141 [D loss: 0.145351, acc.: 95.31%] [G loss: 4.044561]\n",
      "142 [D loss: 0.089322, acc.: 98.44%] [G loss: 4.476583]\n",
      "143 [D loss: 0.113048, acc.: 98.44%] [G loss: 3.362717]\n",
      "144 [D loss: 0.206809, acc.: 85.94%] [G loss: 4.327353]\n",
      "145 [D loss: 0.051088, acc.: 100.00%] [G loss: 4.555828]\n",
      "146 [D loss: 0.346751, acc.: 82.81%] [G loss: 2.980324]\n",
      "147 [D loss: 0.072649, acc.: 96.88%] [G loss: 2.763867]\n",
      "148 [D loss: 0.093478, acc.: 96.88%] [G loss: 3.591234]\n",
      "149 [D loss: 0.042975, acc.: 98.44%] [G loss: 4.033503]\n",
      "150 [D loss: 0.065375, acc.: 98.44%] [G loss: 4.292068]\n",
      "151 [D loss: 0.100862, acc.: 96.88%] [G loss: 3.645230]\n",
      "152 [D loss: 0.071550, acc.: 98.44%] [G loss: 3.636166]\n",
      "153 [D loss: 0.110754, acc.: 98.44%] [G loss: 4.128860]\n",
      "154 [D loss: 0.097048, acc.: 96.88%] [G loss: 3.886378]\n",
      "155 [D loss: 0.081000, acc.: 98.44%] [G loss: 4.068409]\n",
      "156 [D loss: 0.109772, acc.: 98.44%] [G loss: 4.034627]\n",
      "157 [D loss: 0.199515, acc.: 90.62%] [G loss: 4.319787]\n",
      "158 [D loss: 0.079495, acc.: 98.44%] [G loss: 4.513473]\n",
      "159 [D loss: 0.782699, acc.: 70.31%] [G loss: 2.865155]\n",
      "160 [D loss: 0.091653, acc.: 95.31%] [G loss: 4.172418]\n",
      "161 [D loss: 0.050738, acc.: 100.00%] [G loss: 4.323985]\n",
      "162 [D loss: 0.108909, acc.: 96.88%] [G loss: 3.561553]\n",
      "163 [D loss: 0.090759, acc.: 93.75%] [G loss: 3.704488]\n",
      "164 [D loss: 0.146777, acc.: 96.88%] [G loss: 4.248434]\n",
      "165 [D loss: 0.194029, acc.: 92.19%] [G loss: 3.595724]\n",
      "166 [D loss: 0.114093, acc.: 95.31%] [G loss: 5.147513]\n",
      "167 [D loss: 2.120340, acc.: 28.12%] [G loss: 1.572891]\n",
      "168 [D loss: 0.844031, acc.: 70.31%] [G loss: 1.364583]\n",
      "169 [D loss: 0.376888, acc.: 82.81%] [G loss: 3.216729]\n",
      "170 [D loss: 0.072846, acc.: 98.44%] [G loss: 4.536638]\n",
      "171 [D loss: 0.072816, acc.: 98.44%] [G loss: 4.355385]\n",
      "172 [D loss: 0.132878, acc.: 95.31%] [G loss: 4.256103]\n",
      "173 [D loss: 0.100679, acc.: 98.44%] [G loss: 3.837035]\n",
      "174 [D loss: 0.334264, acc.: 84.38%] [G loss: 3.530019]\n",
      "175 [D loss: 0.108701, acc.: 96.88%] [G loss: 4.294187]\n",
      "176 [D loss: 0.444778, acc.: 75.00%] [G loss: 2.168987]\n",
      "177 [D loss: 0.258456, acc.: 89.06%] [G loss: 3.025340]\n",
      "178 [D loss: 0.096576, acc.: 96.88%] [G loss: 3.868431]\n",
      "179 [D loss: 0.220400, acc.: 92.19%] [G loss: 3.548156]\n",
      "180 [D loss: 0.145004, acc.: 93.75%] [G loss: 3.698689]\n",
      "181 [D loss: 0.121198, acc.: 96.88%] [G loss: 3.473058]\n",
      "182 [D loss: 0.252489, acc.: 90.62%] [G loss: 3.563939]\n",
      "183 [D loss: 0.544935, acc.: 75.00%] [G loss: 3.256121]\n",
      "184 [D loss: 0.311214, acc.: 82.81%] [G loss: 2.921835]\n",
      "185 [D loss: 0.283861, acc.: 85.94%] [G loss: 3.330718]\n",
      "186 [D loss: 0.338920, acc.: 84.38%] [G loss: 2.399359]\n",
      "187 [D loss: 0.196734, acc.: 89.06%] [G loss: 3.561738]\n",
      "188 [D loss: 0.482596, acc.: 82.81%] [G loss: 1.715890]\n",
      "189 [D loss: 0.269966, acc.: 82.81%] [G loss: 3.296756]\n",
      "190 [D loss: 0.140249, acc.: 95.31%] [G loss: 3.981137]\n",
      "191 [D loss: 0.458426, acc.: 79.69%] [G loss: 2.097896]\n",
      "192 [D loss: 0.282322, acc.: 85.94%] [G loss: 3.898247]\n",
      "193 [D loss: 0.239953, acc.: 93.75%] [G loss: 3.879122]\n",
      "194 [D loss: 0.442040, acc.: 75.00%] [G loss: 3.737032]\n",
      "195 [D loss: 0.192409, acc.: 92.19%] [G loss: 3.767661]\n",
      "196 [D loss: 0.401778, acc.: 79.69%] [G loss: 3.236423]\n",
      "197 [D loss: 0.183805, acc.: 96.88%] [G loss: 3.618743]\n",
      "198 [D loss: 0.446833, acc.: 73.44%] [G loss: 2.805663]\n",
      "199 [D loss: 0.179470, acc.: 93.75%] [G loss: 3.768673]\n",
      "200 [D loss: 0.267978, acc.: 84.38%] [G loss: 2.826999]\n",
      "201 [D loss: 0.255841, acc.: 89.06%] [G loss: 4.038639]\n",
      "202 [D loss: 0.929840, acc.: 50.00%] [G loss: 2.092879]\n",
      "203 [D loss: 0.148491, acc.: 93.75%] [G loss: 3.337621]\n",
      "204 [D loss: 0.270307, acc.: 90.62%] [G loss: 2.795557]\n",
      "205 [D loss: 0.185795, acc.: 92.19%] [G loss: 3.416046]\n",
      "206 [D loss: 0.787878, acc.: 56.25%] [G loss: 2.393220]\n",
      "207 [D loss: 0.192469, acc.: 89.06%] [G loss: 4.096310]\n",
      "208 [D loss: 1.046188, acc.: 53.12%] [G loss: 1.332969]\n",
      "209 [D loss: 0.296227, acc.: 81.25%] [G loss: 3.213408]\n",
      "210 [D loss: 0.171335, acc.: 95.31%] [G loss: 3.295909]\n",
      "211 [D loss: 0.613557, acc.: 65.62%] [G loss: 2.175327]\n",
      "212 [D loss: 0.253961, acc.: 85.94%] [G loss: 3.501786]\n",
      "213 [D loss: 0.833131, acc.: 51.56%] [G loss: 1.888906]\n",
      "214 [D loss: 0.161262, acc.: 100.00%] [G loss: 2.817733]\n",
      "215 [D loss: 0.316326, acc.: 84.38%] [G loss: 2.553409]\n",
      "216 [D loss: 0.283929, acc.: 85.94%] [G loss: 3.165293]\n",
      "217 [D loss: 0.572523, acc.: 73.44%] [G loss: 2.187135]\n",
      "218 [D loss: 0.236185, acc.: 92.19%] [G loss: 3.500854]\n",
      "219 [D loss: 0.593334, acc.: 67.19%] [G loss: 2.267911]\n",
      "220 [D loss: 0.183974, acc.: 93.75%] [G loss: 3.414868]\n",
      "221 [D loss: 0.434220, acc.: 79.69%] [G loss: 2.470119]\n",
      "222 [D loss: 0.200047, acc.: 95.31%] [G loss: 3.180617]\n",
      "223 [D loss: 1.242004, acc.: 46.88%] [G loss: 1.328485]\n",
      "224 [D loss: 0.377245, acc.: 75.00%] [G loss: 3.382440]\n",
      "225 [D loss: 1.074910, acc.: 39.06%] [G loss: 1.478970]\n",
      "226 [D loss: 0.252384, acc.: 87.50%] [G loss: 2.666831]\n",
      "227 [D loss: 0.593029, acc.: 70.31%] [G loss: 1.621815]\n",
      "228 [D loss: 0.377896, acc.: 81.25%] [G loss: 2.890086]\n",
      "229 [D loss: 1.124348, acc.: 45.31%] [G loss: 1.388727]\n",
      "230 [D loss: 0.408350, acc.: 78.12%] [G loss: 2.934895]\n",
      "231 [D loss: 0.607753, acc.: 68.75%] [G loss: 1.554082]\n",
      "232 [D loss: 0.333280, acc.: 81.25%] [G loss: 2.571159]\n",
      "233 [D loss: 0.673650, acc.: 60.94%] [G loss: 1.464211]\n",
      "234 [D loss: 0.400332, acc.: 75.00%] [G loss: 2.634331]\n",
      "235 [D loss: 0.493511, acc.: 76.56%] [G loss: 2.054315]\n",
      "236 [D loss: 0.569729, acc.: 68.75%] [G loss: 2.390947]\n",
      "237 [D loss: 0.687432, acc.: 56.25%] [G loss: 1.908306]\n",
      "238 [D loss: 0.429139, acc.: 81.25%] [G loss: 2.304727]\n",
      "239 [D loss: 0.698059, acc.: 65.62%] [G loss: 1.558746]\n",
      "240 [D loss: 0.268147, acc.: 93.75%] [G loss: 2.479529]\n",
      "241 [D loss: 0.706510, acc.: 60.94%] [G loss: 1.586994]\n",
      "242 [D loss: 0.404318, acc.: 76.56%] [G loss: 2.340467]\n",
      "243 [D loss: 0.584720, acc.: 65.62%] [G loss: 2.578842]\n",
      "244 [D loss: 0.669006, acc.: 60.94%] [G loss: 1.476878]\n",
      "245 [D loss: 0.362206, acc.: 82.81%] [G loss: 2.364758]\n",
      "246 [D loss: 0.958168, acc.: 46.88%] [G loss: 1.021588]\n",
      "247 [D loss: 0.349105, acc.: 81.25%] [G loss: 2.098688]\n",
      "248 [D loss: 0.580756, acc.: 76.56%] [G loss: 1.960759]\n",
      "249 [D loss: 0.436253, acc.: 79.69%] [G loss: 2.316501]\n",
      "250 [D loss: 0.586249, acc.: 71.88%] [G loss: 1.863389]\n",
      "251 [D loss: 0.402916, acc.: 82.81%] [G loss: 2.317397]\n",
      "252 [D loss: 0.694863, acc.: 56.25%] [G loss: 1.764179]\n",
      "253 [D loss: 0.459418, acc.: 79.69%] [G loss: 1.965627]\n",
      "254 [D loss: 0.714511, acc.: 56.25%] [G loss: 1.502964]\n",
      "255 [D loss: 0.396661, acc.: 82.81%] [G loss: 2.470628]\n",
      "256 [D loss: 1.116171, acc.: 32.81%] [G loss: 0.932290]\n",
      "257 [D loss: 0.527158, acc.: 67.19%] [G loss: 1.797070]\n",
      "258 [D loss: 0.453185, acc.: 79.69%] [G loss: 1.789458]\n",
      "259 [D loss: 0.652421, acc.: 60.94%] [G loss: 1.695842]\n",
      "260 [D loss: 0.557131, acc.: 65.62%] [G loss: 1.944039]\n",
      "261 [D loss: 0.832391, acc.: 46.88%] [G loss: 0.866656]\n",
      "262 [D loss: 0.559114, acc.: 64.06%] [G loss: 1.319037]\n",
      "263 [D loss: 0.509090, acc.: 67.19%] [G loss: 1.785070]\n",
      "264 [D loss: 0.596065, acc.: 67.19%] [G loss: 1.387447]\n",
      "265 [D loss: 0.516118, acc.: 70.31%] [G loss: 1.540432]\n",
      "266 [D loss: 0.653237, acc.: 59.38%] [G loss: 1.536473]\n",
      "267 [D loss: 0.666327, acc.: 62.50%] [G loss: 1.561467]\n",
      "268 [D loss: 0.656951, acc.: 59.38%] [G loss: 1.578726]\n",
      "269 [D loss: 0.760661, acc.: 48.44%] [G loss: 1.379930]\n",
      "270 [D loss: 0.733330, acc.: 54.69%] [G loss: 1.407174]\n",
      "271 [D loss: 0.711680, acc.: 57.81%] [G loss: 1.256265]\n",
      "272 [D loss: 0.499623, acc.: 76.56%] [G loss: 1.646892]\n",
      "273 [D loss: 0.772835, acc.: 54.69%] [G loss: 1.383924]\n",
      "274 [D loss: 0.611142, acc.: 62.50%] [G loss: 1.300850]\n",
      "275 [D loss: 0.616972, acc.: 62.50%] [G loss: 1.418237]\n",
      "276 [D loss: 0.603909, acc.: 64.06%] [G loss: 1.506710]\n",
      "277 [D loss: 0.840187, acc.: 45.31%] [G loss: 1.139375]\n",
      "278 [D loss: 0.632977, acc.: 59.38%] [G loss: 1.569511]\n",
      "279 [D loss: 0.767962, acc.: 45.31%] [G loss: 1.338572]\n",
      "280 [D loss: 0.788720, acc.: 43.75%] [G loss: 1.157739]\n",
      "281 [D loss: 0.704790, acc.: 48.44%] [G loss: 1.175019]\n",
      "282 [D loss: 0.803596, acc.: 42.19%] [G loss: 1.128219]\n",
      "283 [D loss: 0.654491, acc.: 57.81%] [G loss: 1.295773]\n",
      "284 [D loss: 0.749512, acc.: 42.19%] [G loss: 1.074707]\n",
      "285 [D loss: 0.668500, acc.: 57.81%] [G loss: 1.116302]\n",
      "286 [D loss: 0.772381, acc.: 48.44%] [G loss: 1.001530]\n",
      "287 [D loss: 0.682362, acc.: 56.25%] [G loss: 1.123069]\n",
      "288 [D loss: 0.583938, acc.: 67.19%] [G loss: 1.285963]\n",
      "289 [D loss: 0.910188, acc.: 32.81%] [G loss: 0.830483]\n",
      "290 [D loss: 0.633177, acc.: 54.69%] [G loss: 1.190911]\n",
      "291 [D loss: 0.780739, acc.: 39.06%] [G loss: 1.030304]\n",
      "292 [D loss: 0.688977, acc.: 53.12%] [G loss: 1.083183]\n",
      "293 [D loss: 0.696442, acc.: 51.56%] [G loss: 1.122704]\n",
      "294 [D loss: 0.806123, acc.: 35.94%] [G loss: 0.871613]\n",
      "295 [D loss: 0.688688, acc.: 48.44%] [G loss: 1.021008]\n",
      "296 [D loss: 0.755101, acc.: 46.88%] [G loss: 0.909701]\n",
      "297 [D loss: 0.701807, acc.: 48.44%] [G loss: 1.048132]\n",
      "298 [D loss: 0.750869, acc.: 39.06%] [G loss: 0.888593]\n",
      "299 [D loss: 0.839965, acc.: 34.38%] [G loss: 0.750532]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train(epochs=300, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
